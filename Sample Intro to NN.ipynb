{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ac88a8-dde0-44c8-ac10-c7c981e60671",
   "metadata": {},
   "source": [
    "# <center> Introduction to Neural Networks </center>\n",
    "\n",
    "<center><img src=\"Images/nn_image.jpg\"> </center>\n",
    "    \n",
    "[image by fdecomite](https://www.flickr.com/photos/fdecomite/3238821080) \n",
    "\n",
    "\n",
    "Neural networks are an exciting and flexible class of machine learning models that can and are used for a wide variety of applications.  They can be used for regression, classification, and for generating unique outputs, such as you may have seen with generative AI models.\n",
    "\n",
    "The basic model structure underlies many of the most exciting AI and ML applications such as image recognition, text interpretation, self-driving cars, and much more.  \n",
    "\n",
    "In this lesson we will explore the basic foundations of this incredible and widely varied class of models and you will be able to extend this learning to future applications.\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "Neural networks were first described by neurophysiologis Warren McCulloch and mathematician Walter Pitts, as a model for biological brains.  In 1959 Bernard Widrow and Marcian Hoff of Stanford adapted the idea to create MADALINE, the first neural network put into production to eliminate echoes in phone lines.  It's still in use today! -[Stanford History of Neural Networks](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html)\n",
    "\n",
    "The model is built of many 'neurons'.  Each neuron takes in input features and multiplies them by learned coefficients specific to each feature.  The results are summed together, along with a bias term (similar to an intercept), and finally an activation function is applied. The resulting number is the output of that neuron.\n",
    "\n",
    "You may already know the general model behind these neurons.  They are linear models!  You may already know even know a linear model with an output function, the logistic regression model.  A neuron with a sigmoid activation function is the same as a logistic regression mode.\n",
    "\n",
    "The network consists of layers of these neurons. The neurons in each layer process data in parallel and the outputs of each neuron are passed to the next layer.  The output of the output layer is the model prediction.\n",
    "\n",
    "## Neural Network Structure\n",
    "\n",
    "Neural networks can have many structures, but the simplest is the dense neural network, also called a fully connected neural network or multilayered perceptron.\n",
    "\n",
    "1. **Input Layer**: Every network has an input layer with no weights or bias.  This simply accepts the input features and passes them to the next layers.\n",
    "2. **Output Layer**: Every network must have an output layer.  This layer produces the model prediction.  It should have the same number of nodes as the number of required numbers to be predicted.  This is often one, but can be more.\n",
    "3. **Hidden Layers**: Any network that needs to create a non-linear function must have one or more layers between these.  The layers between input and output are called 'hidden layers'.  This is where the real magic happens and where neural networks gain their flexibility.  This allows them to solve linear problems like a linear regression, or non-linear problems like a decision tree can.  With more layers or more neurons, a model can solve extremely complex problems with very complex decision boundaries or functions between input and output.\n",
    "\n",
    "<font color='green'> Important: The neurons of each layer MUST have the same number of weights as the number of neurons from the previous layer.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45929b-c25b-40ef-bdb9-83389b560f9e",
   "metadata": {},
   "source": [
    "## A Simple Neural Network in Action\n",
    "\n",
    "We can create a very simple neural network in NumPy.  Let's start with the 'iris' dataset for some data.  If you have not encountered this dataset, each row represents the length and width of the petals and the length and width of the sepal, resulting in 4 features.  The goal is to predict the species of each flower.\n",
    "\n",
    "This is usually treated as a multi-class classification problem with 3 possible species.  However, we are just going to use the first 2 species to make this a binary classification problem.  This simplifies the math below some."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd089a54-8e4b-4954-8948-5b75731da9e2",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3bb73-0d87-4974-b66e-abe80bd93028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "np.random.seed(42)\n",
    "\n",
    "## Load the data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcf36a-6e07-4126-a09d-b2221e4c0c90",
   "metadata": {},
   "source": [
    "### Convert to Binary Classification\n",
    "We will convert the multiclass iris problem to just be a binary classification problem by eliminating one of the species from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29caff68-c285-4055-9aab-5b5e6eae79d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the features before filtering is (150, 4)\n",
      "the shape of the features after filtering is (100, 4)\n",
      "The unique labels are [0 1]\n"
     ]
    }
   ],
   "source": [
    "## filter out all of the class 2 species\n",
    "filter = labels != 2\n",
    "print(f'The shape of the features before filtering is {features.shape}')\n",
    "features = features[filter]\n",
    "print(f'the shape of the features after filtering is {features.shape}') \n",
    "labels = labels[filter]\n",
    "print(f'The unique labels are {np.unique(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a3586-a9cb-4541-9c03-247da23f2241",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "\n",
    "We will scale the data between 0 and 1 with min-max scaling.  We will subtract the minimum value of each feature from the values of that feature and then divide them by the maximum value.\n",
    "\n",
    "While not required, neural networks have been shown experimentally to perform better with input data with absolute values less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fbe554-5caa-4055-984e-7736ff945239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71428571, 0.48571429, 0.18571429, 0.01428571],\n",
       "       [0.68571429, 0.41428571, 0.18571429, 0.01428571],\n",
       "       [0.65714286, 0.44285714, 0.17142857, 0.01428571],\n",
       "       [0.64285714, 0.42857143, 0.2       , 0.01428571],\n",
       "       [0.7       , 0.5       , 0.18571429, 0.01428571]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a min/max scaling function\n",
    "def min_max_scale(features):\n",
    "    min = np.min(features)\n",
    "    max = np.max(features)\n",
    "    features = (features - min) / max\n",
    "    return features\n",
    "\n",
    "# Scale the features\n",
    "features = min_max_scale(features)\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ce515-3ba1-4c91-a2e0-b7bb626c7c73",
   "metadata": {},
   "source": [
    "We will create a simple network with two neurons that each take as input the 4 features of this dataset, multiplies them by a coefficient, sums them together with a bias, applies a sigmoid activation, sums the results together, and outputs a model prediction.  Since the model will not yet have been trained, the prediction will be essentially random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adc6cd-e69f-4913-a8c9-0a1fa41e7d1b",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89147bd5-bfdb-4f04-a2ea-f66e9dadd697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.48571429, 0.18571429, 0.01428571])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the first sample from the dataset.  This will be output of our input layer.\n",
    "input_vector = features[0]\n",
    "input_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90deef-8051-43f1-b845-bd61b79b8415",
   "metadata": {},
   "source": [
    "## Hidden Layer\n",
    "Our simple network will have one hidden layer with two neurons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c299f-a99a-4dc9-a687-11965ac9d989",
   "metadata": {},
   "source": [
    "### First Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71458d98-7092-440f-b149-ed32294c6507",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0106d6-98e2-4c9c-b576-27c57eef5c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the weights for the first neuron in the hidden layer.\n",
    "neuron_1_weights = np.random.randn(4)\n",
    "neuron_1_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7dcb7-9539-4eab-9ee0-e602b099ff07",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965e6976-5dd3-4537-868b-725a03ec160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23415337])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias term for the first neuron\n",
    "neuron_1_bias = np.random.randn(1)\n",
    "neuron_1_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1a049-d867-49a6-adcd-46b1aeff2dea",
   "metadata": {},
   "source": [
    "#### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d8ca64-dd7d-4cd6-a89e-3736e6bd8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sigmoid activation function\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995c39b-6803-4372-9006-b289dc36249a",
   "metadata": {},
   "source": [
    "### Putting the pieces of the first neuron together\n",
    "\n",
    "Now that we have the pieces we need we will:\n",
    "\n",
    "1. Multiply the input_vector, or the measurements of the first flower, by the weights of the first neuron\n",
    "2. Sum the results and add the bias term.\n",
    "3. Apply the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fcd787a-26b2-4658-b592-05784c94be3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54872688])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate the output of the first neuron\n",
    "\n",
    "# Multiply the input vector and the first neuron weights\n",
    "x =  input_vector * neuron_1_weights\n",
    "# Sum the resulting vector\n",
    "x = np.sum(x)\n",
    "# add the bias\n",
    "x += neuron_1_bias\n",
    "\n",
    "# and apply the sigmoid activation to the result.\n",
    "neuron_1_output = sigmoid(x)\n",
    "neuron_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdae7f2-cbf5-4319-8e55-fd3cbaa082aa",
   "metadata": {},
   "source": [
    "The output of the first neuron is shown above.  Let's functionalize this process so we can more easily apply it to more neurons.\n",
    "\n",
    "We will allow the user the user to pass the activation function as well, since many different activation functions can be used for a neuron, not just sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d465903e-5ccd-4c18-9723-2b8c06aa0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for a neuron.\n",
    "def neuron(input, weights, bias, activation_function):\n",
    "    x = input * weights\n",
    "    x = np.sum(x)\n",
    "    x += bias\n",
    "    return activation_function(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdfebf-3184-4106-8aea-37da432880d1",
   "metadata": {},
   "source": [
    "### Second Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a029a42b-9b37-4b11-8ab7-64eacfcaa6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23413696  1.57921282  0.76743473 -0.46947439]\n",
      "[0.18182497]\n"
     ]
    }
   ],
   "source": [
    "# Create the weights and bias for the second neuron in the hidden layer\n",
    "neuron_2_weights = np.random.randn(4)\n",
    "neuron_2_bias = np.random.rand(1)\n",
    "\n",
    "print(neuron_2_weights)\n",
    "print(neuron_2_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe9a02a-059f-48fa-8e8e-67fe70887f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71452169])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the output of the second neuron using the neuron function\n",
    "neuron_2_output = neuron(input_vector, \n",
    "                         neuron_2_weights, \n",
    "                         neuron_2_bias, \n",
    "                         sigmoid)\n",
    "\n",
    "neuron_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085ea4e-9a07-409d-b9d6-1f43b54465f0",
   "metadata": {},
   "source": [
    "Finally, the layer output is simply the sum of the outputs of the neurons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8fb037-147e-4e91-93b1-55eb1cc6e53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54872688],\n",
       "       [0.71452169]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_output = np.array([neuron_1_output, neuron_2_output])\n",
    "hidden_layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbdecc-fb70-4163-8df5-d94257719497",
   "metadata": {},
   "source": [
    "## Output Layer\n",
    "Since there are 2 possible species, we could construct our output layer 2 different ways.  We could have 2 neurons in the output layer, and each would output the probability the the flower belonged to the corresponding class: `[prob_class_1, prob_class_2]`.  Or, we could just use one output neuron and that could be the probability that the sample belongs to class 1.  The probability of class 0 would just be the inverse of the probability of class 1.  \n",
    "\n",
    "We will choose the 2nd option for simplicity.  This also allows us to keep using the sigmoid activation.  If we had 2 output neurons we would be better off using a 'softmax' activation function.  Remember, we have many choices in activation functions!\n",
    "\n",
    "### Output Neuron Shape\n",
    "\n",
    "Our hidden layer neurons each had 4 weights because the input layer, or the original input data, passed 4 features.  However, since there are only 2 neurons in the hidden layer, only 2 numbers will be passed to the output layer, one from each neuron.  The output neuron will only have 2 weights.\n",
    "\n",
    "Since there will be only one neuron in the output layer, only one number will be outputted as the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56637f4-5aa2-4a47-a905-cffa3e50180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54256004, -0.57138017])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Output Neuron\n",
    "output_neuron_weights = np.random.randn(2)\n",
    "output_neuron_bias = np.random.randn(1)\n",
    "output_neuron_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196074f-c7ba-4311-b76d-b9733a1ca770",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "The output of the final layer of the model is the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfba94a-8118-4bfc-9040-7ea7af2f0575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27678014])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the output of the output layer\n",
    "model_output = neuron(hidden_layer_output,\n",
    "                          output_neuron_weights,\n",
    "                          output_neuron_bias,\n",
    "                          sigmoid)\n",
    "model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f1b27-cb9c-474a-90fa-37d70ceaedcf",
   "metadata": {},
   "source": [
    "Our model predicted the probability that the flower is class 1.  To output an actual class prediction, we would simply round this number.  This would round to 0, so the model predicts the flower to be class 0.\n",
    "\n",
    "Remember that our weights and biases were randomly set.  This is how neural networks start, before fitting on training data.  This prediction is therefor random and not the product of any kind of learning or fitting at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646e3cac-a1a0-4a08-b9d3-7601d60eb5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.rint(model_output)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1172c0f-ecaf-48f9-842e-7cff8e3c54fd",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37646594-68f6-4817-918f-a46a97baabf0",
   "metadata": {},
   "source": [
    "The simple model we created above took a single sample and applied random weights, biases, and activation functions to create a completely random prediction.\n",
    "\n",
    "### Foward Propagation\n",
    "A true neural network would make many predictions on every sample in the dataset and compare those predictions to the true labels to determine how well it did.  This is called <font color='green'> Forward Propagation </font>.  \n",
    "\n",
    "### Backward Propagation\n",
    "Then It would then apply a learning algorithm called <font color='orange'> Gradient Descent </font> to adjust the values of the weights and biases in order to make a better prediction on the next attempt.  This step is called <font color='red'> Backward Propagation </font>.\n",
    "\n",
    "### Epochs\n",
    "The combination of one <font color='green'> Forward Propagation </font> step and one <font color='red'> Backward Propagation </font> step is called an <font color='pink'> Epoch </font>.  A neural network performs multiple <font color='pink'> Epochs </font> to adjust the weights and biases again and again to converge toward more accurate predictions using <font color='orange'> Gradient Descent </font>.  \n",
    "\n",
    "This is the basic fundamental structure and algorithm of a very simple densely connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb42a0e-1005-4d69-8146-dbc2ee3feb36",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Challenge yourself.  Can you create a single function that will calculate the prediction of a model with an arbitrary number of layers and neurons per layer?  Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14e0bf-5431-4c49-8e55-87197a40846b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2",
   "language": "python",
   "name": "tfgpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
