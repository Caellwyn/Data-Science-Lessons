{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908302d5",
   "metadata": {},
   "source": [
    "# Backward Propagation in Densely Connected Feed-Forward Artificial Neural Networks\n",
    "\n",
    "<center><img src=\"Images/nn_image.jpg\"> </center>\n",
    "    \n",
    "[image by fdecomite](https://www.flickr.com/photos/fdecomite/3238821080)\n",
    "\n",
    "## Review:\n",
    "\n",
    "In the last lesson, you learned about the general structure of artificial neural networks (ANNs) and how data flows through the model, processed by each layer.\n",
    "\n",
    "**Neural networks** are like collections of interconnected **neurons**, separated into **layers**. **Neurons** in each layer process data **simultaneously** in different ways, while the **layers** process data **sequentially**. Each layer's **outputs** become the next layer's **inputs**, culminating in the final layer's output as the **model's prediction**.\n",
    "\n",
    "This lesson focuses on a specific type of ANN: the **densely connected feed-forward network**, also known as a multi-layer perceptron or just a dense neural network.\n",
    "\n",
    "#### Vocabulary:\n",
    "* **Artificial Neural Network (ANN)**: A collection of neurons arranged in layers, processing data to make predictions and learning from its prediction errors.\n",
    "* **Neuron**: The basic processing unit of an ANN. Imagine it as a linear regression model with an activation function applied to its output. Each neuron has weights that multiply each input value and a bias that sums these products.\n",
    "* **Parameters**: The **weights and biases** of a model.  These determine how data is processed toward a prediction and are updated, or tuned, during backward propagation to reduce model errors.\n",
    "* **Layer**: A collection of neurons processing data simultaneously. Each layer passes the outputs of all its neurons to the next. In a densely connected network, each neuron in a layer receives output from every neuron in the previous layer.\n",
    "* **Activation Function**: A function applied to a neuron's output to introduce non-linearity. This is crucial for the network to learn complex relationships between input features and targets.\n",
    "* **Forward Propagation**: The process of a model ingesting data, processing it through each layer sequentially (weighted sums + bias), and finally making a prediction. This step also calculates the prediction error using a loss function.\n",
    "* **Backward Propagation**: The process of adjusting weights and biases using gradient descent to minimize the model's prediction error.\n",
    "* **Gradient Descent**: A technique for calculating the steepest downhill path in a multi-dimensional error surface, guiding the network's weight and bias adjustments to reduce prediction error.\n",
    "* **Loss and Loss Function**: The loss of a model is its prediction error.  The loss function is the function that calculates that error, for example, mean squared error might be used for a regression model or binary cross-entropy might be used for a classification model.\n",
    "* **Epoch**: One cycle of a forward propagation and backward propagation step.  A model may be trained for few or many epochs, but ultimately the number of epochs is determined by creator.\n",
    "\n",
    "#### Real-World Applications: \n",
    "Dense neural networks are used in various domains to make regression and classification predictions, similar to more traditional machine learning models, but can often pick up much more complex signals in the data.  They are also often used as sub-components in more complex kinds of deep learning models to achieve tasks like image recognition, machine translation, and stock market prediction.\n",
    "\n",
    "#### Next Steps: \n",
    "\n",
    "We'll dive deeper into backpropagation, exploring how it uses gradient descent to fine-tune the network's parameters and improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac88a8-dde0-44c8-ac10-c7c981e60671",
   "metadata": {},
   "source": [
    "## How Models Learn from Mistakes: **Gradient Descent**\n",
    "\n",
    "Once a model has made a prediction, how does it learn from its errors and improve its performance? The key algorithm here is gradient descent.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "* **Loss Function:** A function to quantify the difference between predictions and true targets or labels to measure how well a model is performing.\n",
    "* **Loss:** A metric to measure model error.  Lower loss should always mean less error.  This means not all metrics are appropriate for a loss function (such as accuracy or F1-score).\n",
    "* **Loss Landscape:** The loss function creates a multidimensional landscape, where each point represents a different combination of parameter values and its associated loss.\n",
    "* **Global Minimum:** The lowest point in the entire landscape, representing the model's optimal configuration.\n",
    "* **Local Minima:** Valleys in the landscape that aren't the global minimum. The model might get stuck in these, preventing it from reaching its full potential.\n",
    "* **Chain Rule:** In neural networks, the chain rule efficiently calculates gradients through multiple layers during backpropagation.\n",
    "\n",
    "### Gradient Descent Explained\n",
    "\n",
    "Imagine a ball rolling downhill, seeking the lowest point in a landscape. **Gradient descent** operates similarly, guiding a model's parameters (weights and biases) towards the lowest loss in its loss landscape.\n",
    "The **loss function** represents the altitude in this landscape, and the parameters, determine the ball's position laterally.  This landscape is referred to as the **Loss Landscape**\n",
    "**Gradient descent** calculates the gradient of the loss function, which points uphill. The model then takes a step in the opposite direction (downhill) to reduce loss.\n",
    "\n",
    "In the animation below you can see how models with different parameter values, represented by different balls, will take different paths toward minimizing loss.  In fact, you can see that the one model at the back does not end up finding the **global minimum**, but gets stuck in a **local minimum** where no change change in weights equal to the **learning rate** step size will result in a reduction of loss, even though there does exist a better combination of parameter values.  Situations like this can be navigated or mitigated by adjusting the **learning rate**.\n",
    "\n",
    "![Gradient Descient Gif](Images/Gradient_descent.gif)\n",
    "\n",
    "Image by: [Jacopo Bertolotti](https://commons.wikimedia.org/wiki/User:Berto)\n",
    "\n",
    "\n",
    "\n",
    "### Mathematical Details:\n",
    "\n",
    "**Gradient descent update rule**: θ^(t+1) = θ^(t) - η * ∇_θL\n",
    "\n",
    "* ∇_θL represents the gradient of the loss function with respect to a parameters (weight or bias).\n",
    "* η is the learning rate (step size for each change).\n",
    "\n",
    "## Backward Propagation\n",
    "\n",
    "After calculating the gradient of the loss function with respect to the output layer, we use the **chain rule** to propagate the changes backward through the layers.\n",
    "\n",
    "**Chain rule for backpropagation**: ∂L/∂θ_ij = ∂L/∂a_j * ∂a_j/∂z_j * ∂z_j/∂θ_ij (for a 3-layer network with layers A, B, and C)\n",
    "* a_j is the output of neuron j in layer A.\n",
    "* z_j is the weighted sum of inputs to neuron j.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Gradient descent is fundamental to training various machine learning models, including neural networks. By understanding its mechanics, we gain deeper insights into how models learn and optimize their parameters to achieve better performance.\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "Next we will walk through a forward and backward step in a neural network using one sample from the Iris Dataset.  This will show you what a complete model epoch looks like.\n",
    "\n",
    "![forward and backward propagation](Images/forward_and_back_propagation.png)\n",
    "\n",
    "[Image Source](https://www.enjoyalgorithms.com/blog/forward-propagation-in-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45929b-c25b-40ef-bdb9-83389b560f9e",
   "metadata": {},
   "source": [
    "# Completing an ANN model in Code Using NumPy\n",
    "\n",
    "In this section we will create a binary classification neural network and test it on one sample.  In a real application we would create a model that can make predictions on, and learn from, many samples simulaneously.  However, for simplicity, we will start with just here.\n",
    "\n",
    "To gauge our model's ability to learn, we want the model's predicted probability to be as close to the true class of a sample as possible.  We want to see the model's predicted probabilities to get closer to the true label with each epoch of learning.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "In the previous lesson you saw how a model propagates a single training instance forward though it's layers.  This method can be used to perform a forward and backward propagation step for each sample in the training set.  This method is called **Stochastic Gradient Descent**.  It's best practice to sample the training set randomly, with replacement, for this approach.\n",
    "\n",
    "However, in this lesson we will take a different approach: **Batch Gradient Descent**.  In **Batch Gradient Descent** The model trains on all training samples simultaneously.  This tends to be faster, and generates a smoother learning curve for the model, whereas **Stochastic Gradient Descent** can generate noisy, erratic learning.\n",
    "\n",
    "The challenge with **Batch Gradient Descent** is managing the shapes of arrays.  There are many matrix multiplication operations and many different arrays to keep track of.  When creating this kind of model it's important to check the shape of your arrays after each step to ensure they are compatible.\n",
    "\n",
    "However, for this lesson, don't get too caught up in that.  Pay attention to how information flows forward, derivatives are calculated and passed backward through layers of the model using the **Chain Rule** to calculate the gradients to update each weight and bias.  The most important takeaway here is the big picture.\n",
    "\n",
    "## Steps:\n",
    "1. Gather and prepare the data.\n",
    "2. Initialize our neurons in the hidden and output layers.\n",
    "3. Use a forward propagation pass to make a model prediction.\n",
    "4. Calculate the loss using binary cross-entropy.\n",
    "5. Calculate and combine the derivatives of the loss function and final activation functions.\n",
    "6. Propagate the gradient backward from output layer to the hidden layer update the weights.\n",
    "7. Evaluate the tuned model.\n",
    "\n",
    "## Data\n",
    "\n",
    "Once again we will use one of the flowers in the Iris dataset to demonstrate how a model would make a prediction on a single sample, then how it will apply **gradient descent** to backward propagation changes to neuron parameters to reduce the loss in the next epoch.\n",
    "\n",
    "![Iris Image](Images/Iris.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd089a54-8e4b-4954-8948-5b75731da9e2",
   "metadata": {},
   "source": [
    "## 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3bb73-0d87-4974-b66e-abe80bd93028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "## Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a3586-a9cb-4541-9c03-247da23f2241",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "\n",
    "We will scale the data between 0 and 1 with min-max scaling.  We will subtract the minimum value of each feature from the values of that feature and then divide them by the maximum value.\n",
    "\n",
    "While not required, neural networks have been shown experimentally to perform better with input data with absolute values less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbe554-5caa-4055-984e-7736ff945239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63291139, 0.43037975, 0.16455696, 0.01265823],\n",
       "       [0.60759494, 0.36708861, 0.16455696, 0.01265823],\n",
       "       [0.58227848, 0.39240506, 0.15189873, 0.01265823],\n",
       "       [0.56962025, 0.37974684, 0.17721519, 0.01265823],\n",
       "       [0.62025316, 0.44303797, 0.16455696, 0.01265823]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a min/max scaling function\n",
    "def min_max_scale(features):\n",
    "    min = np.min(features)\n",
    "    max = np.max(features)\n",
    "    features = (features - min) / max\n",
    "    return features\n",
    "\n",
    "# Scale the features\n",
    "X_sc = min_max_scale(X)\n",
    "X_sc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ce515-3ba1-4c91-a2e0-b7bb626c7c73",
   "metadata": {},
   "source": [
    "We will create a simple network with two neurons that each take as input the 4 features of this dataset, multiplies them by a coefficient, sums them together with a bias, applies a sigmoid activation, sums the results together, and outputs a model prediction.  Since the model will not yet have been trained, the prediction will be essentially random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adc6cd-e69f-4913-a8c9-0a1fa41e7d1b",
   "metadata": {},
   "source": [
    "## 2. Initialize the Hidden and Output Layers\n",
    "\n",
    "In this step we will define a function that will create two layers for us, a hidden layer and an output layer.  Our function take the number of neurons and the activation function as arguments and output the weights and biases of the layer.\n",
    "\n",
    "We will combine the code from the previous lesson into a few functions for convenience.\n",
    "\n",
    "1. `initialize_neuron()` to create some randomized weights and biases for us.\n",
    "2. `intialize_layer()` To combine the neurons into layers.  The resulting layers will be of the form `[*nodes, activation]` where `*nodes` are all of the nodes for that layer and the activation function is the last element of the list.\n",
    "3. Some activation functions, which also include derivative variations to help with backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1a049-d867-49a6-adcd-46b1aeff2dea",
   "metadata": {},
   "source": [
    "#### Activation Function\n",
    "\n",
    "In order to allow our model to learn non-linear functions, we will apply an activation function to each neuron's output. In this case, we will use a sigmoid activation function that confines the result to between 0 and 1.  Later, you will learn about other activation functions, but this is the one we will use today.\n",
    "\n",
    "We are also going to create a linear activation function, which is the identity function, for completeness.  However, we will not use it in this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981d9e2",
   "metadata": {},
   "source": [
    "### Initialization Functions\n",
    "\n",
    "We will create helper functions, `initialize_neuron()` and `sigmoid()`, `linear()`, and `initialize_layer()` to help us build each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b845f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to initialize a neuron\n",
    "def initialize_neuron(n_weights, zero_bias=True):\n",
    "    \"\"\"Returns a tuple of (weights, bias)\n",
    "    The weights will be a numpy array of size n_weights \n",
    "    with each value a random number taken from a normal distribution\n",
    "    If zero_bias == True, the bias will be zero\n",
    "    Otherwise it will be taken from the same distribution as the weights\"\"\"\n",
    "    weights = np.random.randn(1,n_weights)\n",
    "    if zero_bias:\n",
    "        bias = 0\n",
    "    else:\n",
    "        bias = np.random.randn(1)\n",
    "    return weights, bias\n",
    "\n",
    "## Sigmoid activation function\n",
    "def sigmoid(z, derivative=False):\n",
    "  \"\"\"Apply a sigmoid function\"\"\"\n",
    "  if derivative:\n",
    "      return sigmoid(z) * (1-sigmoid(z))\n",
    "  else:\n",
    "      return 1 / (1 + np.exp(-z))\n",
    "\n",
    "## Default activation function\n",
    "def linear(z, derivative=False):\n",
    "    \"\"\"linear activation, equivalent to the identify function\"\"\"\n",
    "    if derivative:\n",
    "        return 1\n",
    "    else:\n",
    "        return z\n",
    "\n",
    "## Function to initialize a layer.\n",
    "def initialize_layer(n_neurons, n_weights, zero_bias=True, activation=linear):\n",
    "    \"\"\"Returns a list of neurons and an activation function.  Each neuron will be a tuple of (weights, bias)\n",
    "    If an activation function is passed it will be applied, otherwise an idendity function will be applied.\"\"\"\n",
    "    layer = [initialize_neuron(n_weights=n_weights, zero_bias=zero_bias) for _ in range(n_neurons)]\n",
    "    layer.append(activation)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f334ed5",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "\n",
    "The features themselves make up the input layer.  No weights or biases are necessary and we will use the data itself for this layer.  However, it is important to keep in mind that that 4 values (features) will be passed to the hidden layer.  This determines the number of weights for each node in that layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ea82a",
   "metadata": {},
   "source": [
    "### Hidden Layer\n",
    "\n",
    "We will create a hidden layer with 2 neurons and 4 weights per neuron.  In a densely connected model each neuron should have a weight for every incoming input.  Since the input layer is sending in 4 values (the features of the data), each neuron needs 4 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c817ca4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986]]), 0),\n",
       " (array([[-0.23415337, -0.23413696,  1.57921282,  0.76743473]]), 0),\n",
       " <function __main__.sigmoid(z, derivative=False)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the hidden layer\n",
    "hidden_layer = initialize_layer(n_neurons=2, n_weights=X.shape[1], zero_bias=True, activation=sigmoid)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4209e9",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "Our output layer will be one neuron because we want the model to make a single value prediction for each sample.  In other situations we might want an output layer with multiple neurons if we wanted the model to output multiple values, such as with multiclass classification or some applications of generative AI.\n",
    "\n",
    "Since the hidden layer has 2 neurons and each neuron will send one value, the neuron in the output layer should contain 2 weights, one for each incoming value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9644385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.46947439,  0.54256004]]), 0),\n",
       " <function __main__.sigmoid(z, derivative=False)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the output layer\n",
    "output_layer = initialize_layer(n_neurons=1, n_weights=len(hidden_layer)-1, zero_bias=True, activation=sigmoid)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf629a7",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Let's put the layers together into a model, which will be a list of the layers and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b48aaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(array([[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986]]), 0),\n",
       "  (array([[-0.23415337, -0.23413696,  1.57921282,  0.76743473]]), 0),\n",
       "  <function __main__.sigmoid(z, derivative=False)>],\n",
       " [(array([[-0.46947439,  0.54256004]]), 0),\n",
       "  <function __main__.sigmoid(z, derivative=False)>]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = [hidden_layer, output_layer]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd95a2",
   "metadata": {},
   "source": [
    "![4-2-1 network](Images/4-2-1%20network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf805d",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation\n",
    "\n",
    "In this step we will use the layers we created to process the data from the input features.  First the weights and biases of the hidden layer neurons will be applied, then the resulting outputs of those neurons will be passed to the output layer.  Finally, a model prediction will be created.  This prediction will the model's estimated probability of the sample belonging to class 1.\n",
    "\n",
    "To help us with this step, we will define a forward pass function that will apply each layer and it's activation function in sequence to produce a model output.  \n",
    "\n",
    "The forward pass function will also store the outputs of each neurons both before and after the activation functions in lists.  These will be used later during backward propagation.\n",
    "\n",
    "Traditionally, the outputs of neurons before before activation are labeled as 'z' and after activations are labeled 'a'.  We will use these conventions to build our dictionary.\n",
    "\n",
    "Let's walk through this:\n",
    "\n",
    "Our model is represented as a list of layers.  Each layer is a list of neurons and an activation function.  Each neuron is a tuple of an array of weights and a bias.\n",
    "\n",
    "### Functions\n",
    "\n",
    "`forward_neuron()` will apply the weights and bias to the neuron inputs.\n",
    "\n",
    "`forward_layer()` will apply the neurons of each layer to the input and store the results.  Then it will apply the activation function to the outputs of each neuron and store the results of that.  It returns a tuple of activations As (after activation function) and pre-activation function neuron outputs, Zs.\n",
    "\n",
    "`forward_model()` will apply the layers of the model to the inputs in order, with the activations of one layer being the inputs of the next layer.  It will store the activations and the neuron inputs of each neuron of each layer in a dictionary.  During backward propagation, we will work backward through those lists to change the weights of each neuron in each layer of the model.  This will include the initial inputs as well, as we will need those to update the first layer, and these are the outputs of the input layer.\n",
    "\n",
    "The final activation of the list of activations in the dictionary is the model prediction, since it represents the activation of the neuron(s) in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d465903e-5ccd-4c18-9723-2b8c06aa0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function for a neuron.\n",
    "def forward_neuron(neuron, input):\n",
    "    \"\"\"Outputs the sum of the bias and the dot product of the weights and inputs.\n",
    "    This is equivalent to multiplying the weights by the inputs and summing the result.\"\"\"\n",
    "    weights, bias = neuron\n",
    "    z = np.dot(weights, input.T) + bias\n",
    "    return z\n",
    "\n",
    "## Define function for a layer.\n",
    "def forward_layer(layer, input):\n",
    "    \"\"\"Performs a forward pass with each neuron in a layer and applies the activation function to each result.\n",
    "    Returns a tuple of activations (Ax) and pre-activation neuron outputs (Zs)\"\"\"\n",
    "    activation = layer[-1]\n",
    "    neurons = layer[:-1]\n",
    "    Zs = np.zeros((len(input),len(neurons)))  # Pre-allocate Zs with expected size\n",
    "    As = np.zeros((len(input),len(neurons)))  # Pre-allocate As with expected size\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        z = forward_neuron(neuron, input)\n",
    "        a = activation(z)\n",
    "        Zs[:,i] = z  # Assign values using indexing\n",
    "        As[:,i] = a\n",
    "    return As, Zs\n",
    "\n",
    "## Define function for model\n",
    "def forward_model(layers, input):\n",
    "    \"\"\"Performs forward passes for each layer in the model.  \n",
    "    Returns an ordered list of layer activations and layer neuron outputs\n",
    "    for each layer\n",
    "    Note that the final activations of the list of layer activations will be the model predictions.\"\"\"\n",
    "    outputs = {'As': [input], 'Zs': [input]}\n",
    "    for i, layer in enumerate(layers):\n",
    "        As, Zs = forward_layer(layer, input)\n",
    "        outputs['As'].append(As)  # Append lists directly\n",
    "        outputs['Zs'].append(Zs)\n",
    "        input = Zs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b8a6a",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Let's take a look at our model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c6890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45820865],\n",
       "       [0.4614505 ],\n",
       "       [0.4615873 ],\n",
       "       [0.46639715],\n",
       "       [0.45914561]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examine the outputs of the model.\n",
    "outputs = forward_model(model, X_sc)\n",
    "predictions = outputs['As'][-1]\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a5a7d",
   "metadata": {},
   "source": [
    "Note that the model predictions are all fairly similar and should not have any very meaningful pattern.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdae7f2-cbf5-4319-8e55-fd3cbaa082aa",
   "metadata": {},
   "source": [
    "## 4. Calculate Loss: Binary Cross-entropy\n",
    "\n",
    "Since this is a binary classification model, we will use binary cross-entropy to calculate the loss.  Remember, we want as low a loss as possible, so our goal will be to decrease this loss in the next epoch.  This will not always immediate directly correlate to improved accuracy, precision, or recall, but as the loss decreases, those metrics will generally tend to improve.\n",
    "\n",
    "\n",
    "Here's the formula for binary cross-entropy:\n",
    "\n",
    "BCE = -(y * log(p) + (1 - y) * log(1 - p))\n",
    "\n",
    "**where**:\n",
    "\n",
    "* BCE: Binary Cross-Entropy loss\n",
    "\n",
    "* y: True label (either 0 or 1)\n",
    "\n",
    "* p: Predicted probability (a value between 0 and 1)\n",
    "\n",
    "* log: Logarithm function (usually base 2 for information theory, but base e is common in machine learning)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Measures the difference between the true distribution (y) and the predicted distribution (p).\n",
    "* Assigns a higher penalty for more confident but incorrect predictions.\n",
    "* Aims to minimize the loss during training, leading to better predictions.\n",
    "\n",
    "**Key Points**:\n",
    "\n",
    "Commonly used for binary classification problems.\n",
    "Often used with the sigmoid activation function in the final layer of a neural network.\n",
    "Can be extended to multi-class classification using categorical cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf14e0bf-5431-4c49-8e55-87197a40846b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796196578972339"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Binary cross-entropy in NumPy\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1,1)\n",
    "    \"\"\"Calculates the binary cross-entropy loss.\"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Prevent numerical instability\n",
    "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "loss = binary_crossentropy(y, predictions)\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f26606",
   "metadata": {},
   "source": [
    "We will see if we can reduce this loss, which means that the predicted probabilies are closer to the actual class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b22c6",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "In the following steps we will calculate the derivative of the function from model to loss.  In order to do this we will use the chain rule to propagation derivatives backward from the end (loss) to the beginning (hidden layer parameters) and adjust the weights and biases of each layer using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44007ac4",
   "metadata": {},
   "source": [
    "## 5. Calculate Derivatives of Loss and Sigmoid Activation\n",
    "\n",
    "We will start with functions to calculate the derivative of the loss and sigmoid activations and combine them into one final loss derivative to perform gradient descent on the output layer.\n",
    "\n",
    "These will calculate derivatives for each of the model predictions and true labels to pass back to the model.  The model will update based on the average of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb5ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary cross-entropy derivative function\n",
    "def binary_crossentropy_activation_derivative(y_true, z, activation):\n",
    "    \"\"\"Calculates the derivative of the binary cross-entropy loss and final activation.\"\"\"\n",
    "    ## Reshape for consistent array sizes\n",
    "    y_true = y_true.reshape(-1,1)\n",
    "\n",
    "    ## Apply activation function to the final node output\n",
    "    y_pred = activation(z)\n",
    "\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Prevent numerical instability\n",
    "\n",
    "    ## Calculate the derivative of the BCE loss function\n",
    "    bce_deriv = -(y_true / y_pred) + (1 - y_true) / (1 - y_pred) # BCE deriviative\n",
    "\n",
    "    ## Calculate the derivative of the activation function\n",
    "    activatation_deriv = activation(z, derivative=True) # Activation function derivative\n",
    "\n",
    "    ## Combine the BCE and activation derivatives using the chain rule\n",
    "\n",
    "    final_deriv = bce_deriv * activatation_deriv # Chain rule\n",
    "\n",
    "    ## Ensure proper shape of derivatives\n",
    "    if len(final_deriv.shape) < 2:\n",
    "        final_deriv = final_deriv[np.newaxis,:]\n",
    "    return final_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812b787",
   "metadata": {},
   "source": [
    "## 6. Update the weights and bias of the layers\n",
    "\n",
    "Now comes the model learning!  We start at the end of the model, the output layer, and propagate the changes in parameters upward, calculating the new gradient between each layer.\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "We generally want our models to slowly converge toward the global minimum so they don't go too far!  For this reason we define a learning rate to slow down the learning and allow the model to correctly traverse the loss landscape.  This is a parameter that needs careful tuning, but it's generally a small number that is multipled times the gradient.\n",
    "\n",
    "### Functions:\n",
    "\n",
    "`update_node()` Our first function updates the weights and biases of the nodes.  We first calculate the gradient for the weights by multiplying the node output gradient with the node's inputs and take the mean of the gradient for each input.  This gives us the gradient of the function represented by that node, f(inputs) by the chain rule.\n",
    "\n",
    "`calculate_previous_layer_gradient()` We also need to calculate the derivative of each node in this layer to pass back to the previous layer.  We do this by using, `np.dot()`, on the weights and current layer derivatives.  \n",
    "\n",
    "`update_layer_and_calculate_gradient()` The final function combines the first two and applies them to each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc09abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_node(weights, bias, learning_rate, next_layer_gradient, node_inputs):\n",
    "    \"\"\"Updates the weights and bias of a node in a neural network using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        weights: A NumPy array of the node's weights.\n",
    "        bias: A NumPy scalar representing the node's bias.\n",
    "        learning_rate: The learning rate for gradient descent.\n",
    "        next_layer_gradient: A NumPy array of the gradient received from the subsequent layer.\n",
    "        node_inputs: A NumPy array of the inputs to the node.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the updated weights and bias.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate gradient for weights\n",
    "    weight_gradient = next_layer_gradient.reshape(-1,1) * node_inputs\n",
    "    weight_gradient = weight_gradient.mean(0)\n",
    "    # Update weights\n",
    "    updated_weights = weights - learning_rate * weight_gradient\n",
    "\n",
    "    # Calculate gradient for bias\n",
    "    bias_gradient = next_layer_gradient.mean()  # Average gradient across features\n",
    "\n",
    "    # Update bias\n",
    "    updated_bias = bias - learning_rate * bias_gradient\n",
    "\n",
    "    return updated_weights, updated_bias\n",
    "\n",
    "def calculate_previous_layer_gradient(weights, current_gradient, activation_function=None):\n",
    "    \"\"\"Calculates the gradient for the previous layer in a neural network.\n",
    "\n",
    "    Args:\n",
    "        weights: A NumPy array of the weights connecting the current layer to the previous layer.\n",
    "        current_gradient: A NumPy array of the gradient for the current layer.\n",
    "        activation_function: The activation function used in the previous layer (optional).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of the gradient for the previous layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate gradient before activation (if applicable)\n",
    "    current_gradient = current_gradient.reshape(-1,1)\n",
    "    pre_activation_gradient = np.dot(current_gradient, weights.reshape(1,-1)) ## Matrix multiplication\n",
    "\n",
    "    # Apply activation function derivative (if applicable)\n",
    "    if activation_function is not None:\n",
    "        previous_layer_gradient = activation_function(pre_activation_gradient, derivative=True)\n",
    "    else:\n",
    "        previous_layer_gradient = pre_activation_gradient\n",
    "    return previous_layer_gradient\n",
    "\n",
    "def update_layer_and_calculate_gradient(layer, layer_inputs, next_layer_gradient, learning_rate):\n",
    "    \"\"\"Updates a layer's weights and biases using gradient descent and calculates the gradient for the previous layer.\n",
    "\n",
    "    Args:\n",
    "        layer: A list of nodes, where each node is [weights, bias]. The activation function is appended to the end of the list.\n",
    "        layer_inputs: A NumPy array of the inputs to the layer.\n",
    "        next_layer_gradient: A NumPy array of the gradient received from the subsequent layer.\n",
    "        learning_rate: The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the updated layer and the gradient for the previous layer.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_nodes = []\n",
    "    previous_layer_gradients = np.zeros((layer_inputs.shape[0], len(layer)))\n",
    "\n",
    "    # Iterate through nodes in the layer\n",
    "    for node_index, node in enumerate(layer[:-1]):  # Exclude the activation function\n",
    "        print(f'Updating node {node_index}')\n",
    "        weights, bias = node\n",
    "        # Update node weights and bias\n",
    "        updated_weights, updated_bias = update_node(\n",
    "            weights, bias, learning_rate, next_layer_gradient[:, node_index], layer_inputs\n",
    "        )\n",
    "        # Append updated node to the new layer\n",
    "        updated_nodes.append((updated_weights, updated_bias))\n",
    "\n",
    "        # Calculate gradient for the previous layer (if not the first layer)\n",
    "        previous_layer_gradients[:, node_index] = calculate_previous_layer_gradient(weights, \n",
    "                                                                    next_layer_gradient[:, node_index], \n",
    "                                                                    activation_function=layer[-1])[:,0]\n",
    "\n",
    "\n",
    "    # Append activation function to the updated layer\n",
    "    updated_nodes.append(layer[-1])\n",
    "\n",
    "    return updated_nodes, previous_layer_gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd017df8",
   "metadata": {},
   "source": [
    "## `backward_model()`\n",
    "Our final backward propagation function will start with the final loss and activation derivatives and propagate the gradients backward through the layers of the model from end to beginning.  For this purpose, the function will create a reversed version of the model to iterate through (rather than iterating in reverse).  \n",
    "\n",
    "![forward and backward propagation](Images/forward_and_back_propagation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a24402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_model(model, y_true, outputs, learning_rate):\n",
    "    \"\"\"Perform backward propagation on a model.\n",
    "    1. Calculate gradients for the output layer\n",
    "    2. Apply gradient descent to layers using backpropagation using the chain rule\n",
    "    Args:\n",
    "    model: The model to perform backpropagation on\n",
    "    y_true: The true labels\n",
    "    outputs: the dictionary of outputs of each layer from the forward_model() function\n",
    "    learning_rate: The learning rate to apply to each gradient during gradient descent to control step size.  Recommend between .1 and .0001\n",
    "    \n",
    "    Returns: New model with backward propagation applied\"\"\"\n",
    "\n",
    "    \n",
    "    final_z = outputs['Zs'][-1]\n",
    "    final_activation = model[-1][-1]\n",
    "    gradient = binary_crossentropy_activation_derivative(y_true,\n",
    "                                                        final_z,\n",
    "                                                        final_activation)\n",
    "    \n",
    "    ## Reverse the models and the\n",
    "    new_model = []\n",
    "    reversed_model = model.copy()\n",
    "    reversed_model.reverse()\n",
    "    outputs['As'].reverse()\n",
    "    outputs['Zs'].reverse()\n",
    "    for i, layer in enumerate(reversed_model):\n",
    "        print(f'Updating layer {i}')\n",
    "        layer_inputs = outputs['As'][i+1]\n",
    "        new_layer, gradient = update_layer_and_calculate_gradient(layer,\n",
    "                                                                  layer_inputs=layer_inputs,\n",
    "                                                                  next_layer_gradient=gradient,\n",
    "                                                                  learning_rate=learning_rate)\n",
    "        \n",
    "        new_model.append(new_layer)\n",
    "    \n",
    "    new_model.reverse()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be7f07",
   "metadata": {},
   "source": [
    "## Evaluate the Tuned Model\n",
    "\n",
    "Now that we've trained our model, has our model successfully learned anything?  Let's evaluate it using our loss function to find out.  We will first evaluate our untrained model for a baseline, then compare it to the newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776d150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE of model with no training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6796196578972339"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the model with no training using binary cross-entropy\n",
    "outputs1 = forward_model(model, X_sc)\n",
    "pred1 = outputs1['As'][-1]\n",
    "print('BCE of model with no training')\n",
    "binary_crossentropy(y, pred1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4bb6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating layer 0\n",
      "Updating node 0\n",
      "Updating layer 1\n",
      "Updating node 0\n",
      "Updating node 1\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model after one epoch of learning\n",
    "\n",
    "## Set learning rate (should be tuned)\n",
    "learning_rate=.1\n",
    "\n",
    "## Perform backward propagation on the model\n",
    "model2 = backward_model(model, y, outputs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcf8eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE of model after 1 epoch of training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5910228563325499"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the new model\n",
    "outputs2 = forward_model(model2, X_sc)\n",
    "pred2 = outputs2['As'][-1]\n",
    "print('BCE of model after 1 epoch of training')\n",
    "binary_crossentropy(y, pred2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69affe04",
   "metadata": {},
   "source": [
    "The model learned to more accurately classify iris flowers using various measurements of their petals and sepals!\n",
    "\n",
    "![Brain](Images/256px-Human_Brain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a4f01",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We've built a small neural network capable of forward and backward propagation.  It iteratively learns to better predict the class labels by reducing the loss after each epoch of training.  It uses gradient descent to change the weights of each node in each layer according to the derivative of the loss function by applying the chain rule to propagate the calculate derivative backward to each layer.\n",
    "\n",
    "Repeated epochs of forward and backward propagation should continue to reduce the loss...to a point.  Real world datasets nearly always have some irreducible noise.\n",
    "\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "1. Create validation data and check for overfitting.  Save the loss at reach epoch and plot them.  Is there a number of epochs where the model starts to overfit?\n",
    "2. Create a loop to train a model for n number of epochs\n",
    "3. Convert this functional approach to an object-oriented approach.  A model should have forward and backward propagation methods, a training method that combines them, and keep interal variables of the outputs of each layer during a forward step for use during backward propagation, and the loss after each epoch for plotting after training.\n",
    "4. Implement mini-batching by breaking the dataset into smaller batchs and training on each one in sequence, perhaps try batches of 10 or so.  This is a combination of **Stochastic** and **Batch Gradient Descent**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
